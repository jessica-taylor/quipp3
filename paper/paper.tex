\documentclass{article}
\usepackage{cite}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{array}
% \usepackage{minted1}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{multicol} \usepackage{fancyheadings} \usepackage{pdfpages}
\usepackage{nips15submit_e,times}
\setlength{\emergencystretch}{10em}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
 
\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\DeclareMathOperator*{\Exp}{\mathbb{E}}
\DeclareMathOperator*{\Prob}{\mathbf{P}}


\lstdefinelanguage{JavaScript}{
      morekeywords={break, case, catch, continue, debugger, default, delete,         do, else, false, finally, for, function, if, in, instanceof, new, null, return, switch, this, throw, true, try, typeof, var, void, while, with},
      morecomment=[s]{/*}{*/},
      morecomment=[l]//,
      morestring=[b]",
      morestring=[b]'
    }
  \lstset{language=JavaScript, xleftmargin=0.5in}

\title{Qualitative Probabilistic Programming}


\author{
Jessica Taylor \\
Department of Computer Science\\
Stanford University \\
Stanford, CA 94305
\texttt{jessica.liu.taylor@gmail.com}
\And
Andreas Stuhlm\"uller \\
Department of Brain and Cognitive Sciences \\
MIT \\
Cambridge, MA 02139 \\
\texttt{andreas@stuhlmueller.org}
\And
Noah Goodman \\
Department of Psychology \\
Stanford University \\
Stanford, CA 94305
\texttt{ngoodman@stanford.edu}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

  \maketitle

  \begin{abstract}
    When writing probabilistic programs, sometimes it is difficult for the programmer to specify the correct
    parameterized family of distributions.  We explore an extension to
    probabilistic programming languages that allows programmers to mark some
    distributions as unspecified.  Then, for each distribution, the system can
    determine some reasonable family of distributions and infer maximum likelihood
    parameters.
  \end{abstract}


  \section{Introduction}

  By separating model specification and inference, probabilistic programming 
  has made it easier for non-experts to implement and use probabilistic models.
  Practitioners frequently have strong intuitions about the {\em structure}
  of their domain knowledge, such as which latent variables exist and what
  their causal relations are, and probabilistic programming allows them to encode
  this knowledge. However, it also requires them to specify the specific parametric
  shape and parameterization of any distributions used, and intuitions tend to
  be much less precise there.
  We present Quipp, a system that does {\em not} require such specification;
  instead, random variables and random functions can be left undefined
  and will automatically be filled in under maximum entropy assumptions
  based on their types and available datasets.
  
  Our formalism can concisely express a wide variety of models that machine
  learning practitioners care about, and we provide an expectation
  maximization algorithm that can learn the parameters for many of
  these models with reasonable efficiency. This system makes it easy
  for non-experts to encode their beliefs about the data and to get
  predictions based on as few additional assumptions as possible.
  
  % This feature has multiple advantages.  First, it is easier to write
  % models without knowing about the class of models being used.  This should
  % make probabliistic programming more accessible to non-experts.  Secondly,
  % parameter inference is more efficient if specialized algorithms are used
  % rather than the generic algorithms used to infer other random variables
  % (such as Metropolis Hastings).

  % We define an example probabilistic programming language with this feature
  % (Quipp) and show how it can be used to write machine learning models
  % concisely.

  % In an ordinary probabilistic programming language (such as Church),
  % it is possible to treat parameters as random variables.  This
  % would allow ordinary inference algorithms to infer parameters.  However,
  % there are advantages of having unknown functions as a feature
  % in the language.
  % First, it is easier to
  % write programs without knowing the details of different parameterized distributions.
  % Second, the system can use specialized algorithms to infer parameters faster.

  % Inference in these models is performed using the expectation-maximization
  % algorithm, with alternating steps of inferring latent variables and optimizing
  % parameters.

  % - Motivation for Quipp
  %   - Explanation of "unknown functions"
  %   - Writing machine learning algorithms as probabilistic programs
  %   - Accessibility to non-experts
  %   - Comparison to existing probabilistic programming languages
  %     - In other languages, use random variables for parameters
  %     - Random variables slower because they are updated independently

  In the following, we first specify the syntax used to write Quipp programs,
  including the notation for unknown variables and functions.
  We describe the class of exponential family variables and functions that our system can learn,
  and present the expectation maximization algorithm used to learn them.
  We then demonstrate the expressiveness of our language, and the broad
  applicability of our algorithm, by writing some of the most common machine learning models
  in Quipp: clustering, naive Bayes, factor analysis, a Hidden Markov model and Latent Dirichlet Allocation.
  
  \section{Syntax}

  Quipp is implemented as a library for webppl programs.  Webppl \cite{dippl} is a probabilistic programming language
  that is similar to Javascript but also contains features for generating random values, conditioning on values,
  and estimating expectations. Quipp programs are written as webppl programs that have access to additional special functions.

  Here is an example of a Quipp program to cluster 2d points into 2 clusters:

  {
  \begin{lstlisting}
var Cluster = Categorical(2);
var Point = Vector(2, Double);
var getPoint = randFunction(Cluster, Point);

var model = function() {
  var cluster = randomValue(Cluster);
  observe(getPoint, cluster);
};
  \end{lstlisting}
}

  We declared two types (\texttt{Point} and \texttt{Cluster}) and one
  random function (\texttt{getPoint}).  Type annotations are necessary for random
  functions.  The type \texttt{Vector(2, Double)} expresses the fact that the
  points are vectors in $\mathbb{R}^2$, and the type \texttt{Categorical(2)} expresses the
  fact that there are 2 possible clusters (so a cluster may be either 0 or 1).

  The variable \texttt{model} specifies a generative model for a single data point.
  We use the \texttt{randomValue} function to generate a random uniform cluster, and then
  use \texttt{getPoint} to generate the point given the cluster.  Since \texttt{getPoint}
  is an unknown random function, we will need to infer its parameters.
  The \texttt{observe} function allows us to observe data; here it says that our
  observation consist of the result of the call \texttt{getPoint(cluster)}.

  To demonstrate, let us run this example on the Iris dataset, which consists of 150 points \cite{Lichman:2013}.  When we run the program on this data, we infer the parameters to the random function \texttt{getPoint}.
  In this case, \texttt{getPoint} is a linear function with Gaussian noise, so it will naturally
  split the data into 2 clusters with equal variance, as seen in Figure \ref{iris-orig}.

  \begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.5]{../plots/irisclusters_orig.png}
  \end{center}
  \caption{Clustering the Iris dataset}
  \label{iris-orig}
\end{figure}

  The first cluster is at (6.3, 5.0) and the second is at (5.0, 1.6).  They both
  have a standard deviation of 0.54 in the x direction and 0.69 in the y direction.
  We could use these parameters to fill in the generative model, giving us a webppl program:
  {
  \begin{lstlisting}
var model = function() {
  var cluster = randomInteger(nclusters);
  return [gaussian(cluster == 0 ? 6.3 : 5.0, 0.54),
          gaussian(cluster == 0 ? 5.0 : 1.6, 0.69)];
};
  \end{lstlisting}
}
  This model is estimated to assign probability density $e^{-393}$ to the data, yielding a perplexity of $e^{-393/150} = 0.0728$.

  \section{Family of distributions}
  
    
  In the previous example, \texttt{randFunction(Categorical(2), Vector(2, Double))} represented 2 unknown axis-aligned 2-dimensional Gaussian clusters with equal variances.  In general, \texttt{randFunction} returns a randomized function that is a member of some generalized linear model
    determined by the desired argument types and return type.
    The distribution
    of the function's return value $y$
    is some exponential family whose natural
    parameters are determined from the arguments $x$:

    $$p_{\eta}(y | x) = \exp\left(\eta(x)^T \phi(y) - g(\eta(x))\right)$$

    Here, $\eta(x)$ is the natural parameter, $\phi(y)$ is a vector of $y$'s sufficient statistics,
    and $g$ is the log partition function.

    To determine $\eta(x)$, we label
    some subset of the sufficient statistics of both $x$ and $y$ as \emph{features}.  For the gaussian
    distribution, the sufficient statistics are $X$ and $X^2$ but the only feature is $X$.  For the
    categorical distribution \texttt{Categorical(n)}, the sufficient statistics
    and features are both $[X = 1], [X = 2], ..., [X=n - 1]$.
    The natural
    parameters corresponding to non-features are constant, while natural
    parameters corresponding to features are determined as an affine
    function of the features of the arguments.
    Features are selected so they correspond
    to natural parameters that can take on any real number.

    Let $\psi(x)$ be the features of $x$.  Then
  $$\eta(x) = \mathbf{N}^T \begin{bmatrix} 1 \\ \psi(x) \end{bmatrix}$$
    $$p_{\mathbf{N}}(y | x) = \exp\left(\begin{bmatrix} 1 \\ \psi(x) \end{bmatrix} ^T \mathbf{N} \phi(y) - g\left(\mathbf{N}^T \begin{bmatrix} 1 \\ \psi(x) \end{bmatrix}\right)\right)$$

    where $\mathbf{N}$ is a matrix containing our parameters.  It must have 0 for each entry whose row corresponds
    to a sufficient statistic of $y$ that is not a feature and whose column is not 1.
    This ensures that only the
    natural parameters that are features of $y$ are affected by $x$.
    
    Additionally, we provide a $\texttt{randomValue}$ function for convenience, which
    returns a sample from the ``default'' distribution for a given type.  Table \ref{type-table} shows the types
    in Quipp and their corresponding sufficient statistics, features, and English descriptions.

\begin{table}[t]
  \caption{Types in Quipp}
  \label{type-table}
    \begin{center}
    {\scriptsize
      \begin{tabular}{| c | c | c | c | C{3cm} |}
      \hline
      T & $\phi_T(X)$ & $\psi_T(X)$ & \texttt{randomValue(T)} & Random function class \\

      \hline
    \texttt{Double} & $\begin{bmatrix} X \\ X^2 \end{bmatrix}$ & $\begin{bmatrix} X \end{bmatrix}$ & 
      $\mathcal{N}(0, 1)$ &
      Linear regression\\

      \hline
    \texttt{Categorical(n)} & $\begin{bmatrix} [X = 1] \\ [X = 2] \\ ...\\ [X = n-1] \end{bmatrix}$ & $\begin{bmatrix} [X = 1] \\ [X = 2] \\ ...\\ [X = n-1] \end{bmatrix}$ & uniform & $n$-class logistic regression \\

      \hline
    \texttt{Tuple(T1, T2)} & $\begin{bmatrix} \phi_{T1}(X_1) \\ \phi_{T2}(X_2) \end{bmatrix}$
                           & $\begin{bmatrix} \psi_{T1}(X_1) \\ \psi_{T2}(X_2) \end{bmatrix}$ &
      \begin{tabular}{l}
        \texttt{[randomValue(T1),}
       \\
     \texttt{ randomValue(T2)]}
      \end{tabular}
                           &
      Independent regression
      \\
      \hline
    \end{tabular}
  }
\end{center}
\end{table}

  Note that \texttt{Vector(n, T)} is shorthand for \texttt{Tuple([T, T, ..., T])} (with $n$ copies of $T$),
  and \texttt{Bool} is shorthand for \texttt{Categorical(2)}.

  \section{Example: improving the clustering model}
  Using this knowledge about the form of the random functions, we can improve the clustering model from before.
  As observed in Figure \ref{iris-orig}, the two clusters are forced to have the
  same variance. They do not fit the data well, since the data has a different
  shape in each location.  To fix this problem, we can substitute the following
  model, which uses a separate random function (and therefore a separate axis-aligned Gaussian distribution) for each cluster:
  \begin{lstlisting}
var Cluster = Categorical(2);
var Point = Vector(2, Double);
var getPointFunctions = [randFunction(Point), randFunction(Point)];

var model = function() {
  var cluster = randomValue(Cluster);
  observe(getPointFunctions[cluster]);
};
\end{lstlisting}
  \begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.5]{../plots/irisclusters_indep.png}
  \end{center}
  \caption{Clustering the Iris dataset with independent variances}
  \label{iris-indep}
\end{figure}
Using this model, we get the clusters shown in Figure \ref{iris-indep}.
  This model (with the parameters filled in) is estimated to assign probability density $e^{-337}$ to the data, yielding a perplexity of 0.1058.  This is a significantly improved fit.

  The fact that the gaussian distribution for each cluster must be axis-aligned limits the degree to which the model can fit the data.   To allow each cluster to be an arbitrary Gaussian distributions (where the $x$ and $y$ coordinates may be correlated),
  we can use the following model:

\begin{lstlisting}
var Cluster = Categorical(2);
var getX = [randFunction(Double), randFunction(Double)];
var getY = [randFunction(Double, Double), randFunction(Double, Double)];

var model = function() {
  var cluster = randomInteger(nclusters);
  var x = observe(getX[cluster]);
  var y = observe(getY[cluster], x);
};
\end{lstlisting}

Here, there are 2 random functions for each cluster, one to get the $x$ coordinate and one to get the $y$ coordinate (whose distribution may depend linearly on the $x$ coordinate).  This allows $x$ and $y$ to be correlated, improving fit, as seen in Figure \label{iris-dep}.
  This model is estimated to assign probability $e^{-269}$ to the data, yielding a perplexity of 0.166.  This is a large improvement from the previous model.

  \begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.5]{../plots/irisclusters_dep.png}
  \end{center}
  \caption{Clustering the Iris dataset with multivariate Gaussian distributions}
  \label{iris-dep}
\end{figure}




  \section{Inference}

    To infer both latent variables and parameters, we use a Monte Carlo
    expectation maximization algorithm \cite{mcem} on the probabilistic model, iterating stages of
    estimating latent variables using Metropolis Hastings and inferring
    parameters using gradient descent.  The first iteration uses randomly generated parameters.

    For the expectation step, we must estimate latent variable distributions given
    fixed values for the parameters.  To do this, we can replace unknown random functions in the Quipp program
    with random functions set to use these fixed parameter values, yielding a probabilistic program.
    We use the Metropolis Hastings algorithm to perform inference in this program,
    yielding a distribution of execution traces (where each execution trace specifies the result of every
    call to a random function).  Next, for each random function, we can find all calls
    to it in the trace to get the training data.

    For the maximization step, given samples from each random function, we set the
    parameters of the function to maximize the likelihood of the samples.  To do this we,
    we use gradient descent.

    Given $(x, y)$ samples, parameter estimation to maximize log probability is a convex
    problem because the log probability function is concave as a function of $\mathbf{N}$:
    $$\log p_{\mathbf{N}}(y | x) = \begin{bmatrix} 1 \\ \psi(x) \end{bmatrix} ^T \mathbf{N} \phi(y) - g\left(\mathbf{N}^T \begin{bmatrix} 1 \\ \psi(x) \end{bmatrix}\right)$$

    This relies on the fact that $g$ is convex, but this is true in general for any exponential family distribution.
    Since the problem is convex, it is possible to use gradient descent to optimize the parameters.  Although
    the only exponential family distributions we use in this paper are the categorical and Gaussian distributions,
    we can use the same algorithms for other exponential families, such as the Poisson and gamma distributions.

  \section{Evaluation}

    To evaluate performance, for each model, we:
    \begin{itemize}
      \item
        Randomly generate parameters $\theta$
      \item
        Generate datasets $x_{train}, x_{test}$ using $\theta$
      \item
        Estimate $\log P(x_{test} | \theta)$
      \item
        Use the EM algorithm to infer approximate parameters $\hat{\theta}_i$ from $x_{train}$, for iteration $i = 1, 2, ...$
      \item
        Estimate $\log P(x_{test} | \hat{\theta}_i)$
      \item
        Use the previous estimates to estimate the regret $\log P(x_{test} | \hat{\theta}_i) - \log P(x_{test} | \theta)$
    \end{itemize}
    Estimating $\log P(x_{test} | \theta)$ is nontrivial, given that the model contains latent variables.
    We use the Sequential Monte Carlo algorithm for this, as described in \cite{dippl}.  For the following examples,
    for each iteration except the first, we plot quantiles of average regret per sample over multiple runs.  We exclude
    the first iteration because it represents the regret of randomly generated parameters, which is much higher than the regret in future iterations.  Since
    the expectation maximization algorithm does not always converge to a global optimum, we do not expect regret
    to go to 0 as we increase the number of iterations.


  \section{Examples}

\FloatBarrier
  \subsection{Clustering}

{\small
\begin{lstlisting}
var Cluster = Categorical(3);
var Point = Vector(2, Double);
var getPoint = randFunction(Cluster, Point);

var model = function() {
  var cluster = randomValue(Cluster);
  observe(getPoint, cluster);
};
\end{lstlisting}
}


In this example, we cluster 2d points into 3 different clusters.  Given a cluster, the distribution for a point is some independent Gaussian distribution.  This is similar to fuzzy c-means clustering.  Regret usually reduces quite a lot as iterations increase.

  \begin{figure}[h]
\begin{center}
  \includegraphics[scale=0.5]{../plots/accuracy_nd_clustering.png}
\end{center}
\caption{Regret-per-sample quantiles for clustering}
\end{figure}

% We randomly generated parameters for this example 100 times, and each time took 100 samples and then ran 10 EM iterations.  The accuracy is defined as the maximum percentage of points assigned to the correct cluster, for any permutation of clusters.  On average, accuracy increased in each EM iteration, as shown it this graph:
% 
% \begin{center}
% \includegraphics[scale=0.4]{cluster_accuracy.png}
% \end{center}

\FloatBarrier
\subsection{Naive Bayes}

{\small
\begin{lstlisting}
var Class = Categorical(2);
var Features = Vector(10, Bool);
var classFeatures = [randFunction(Features), randFunction(Features)];

var model = function() {
  var whichClass = observe(randomValue, Class);
  observe(classFeatures[whichClass]);
};
\end{lstlisting}
}

The naive Bayes model is similar to the clustering model.  We have two classes and a feature
distribution for each.  Since each feature is boolean, we will learn
a different categorical distribution for each class.  The main difference from clustering is that,
since this is a supervised learning method, we observe the class in addition to the features.  Since there
are no latent variables, the system finds the correct parameters in a single iteration, so the regret plot is uninteresting.

\FloatBarrier
  \subsection{Factor analysis}
{\small
\begin{lstlisting}
var Factors = Vector(2, Double);
var Point = Vector(5, Double);
var getPoint = randFunction(Factors, Point);

var model = function() {
  var factors = randomValue(Factors);
  return observe(getPoint, factors);
};
\end{lstlisting}
}

The factor analysis model is very similar to the clustering model.  The main difference is that we replace the categorical \texttt{ClusterType} type with a vector type.  \texttt{randomValue(Factors)} will return a vector from the standard multivariate normal distribution.  This results in the model attempting to predict each point as an affine function of a vector of standard normal values, with Gaussian noise.  Regret reduces as iterations increase.

  \begin{figure}[h]
\begin{center}
  \includegraphics[scale=0.5]{../plots/accuracy_factor_analysis.png}
\end{center}
\caption{Regret-per-sample quantiles for factor analysis}
\end{figure}

\FloatBarrier
  \subsection{Hidden Markov model}
{\small
\begin{lstlisting}
var chainLength = 20;
var State = Categorical(2);
var Obs = Categorical(4);
var transFun = randFunction(State, State);
var obsFun = randFunction(State, Obs);

var observeStates = function(startState, i) {
  if (i == chainLength) {
    return [];
  } else {
    observe(obsFun, startState);
    observeStates(transFun(startState), i+1);
  }
};

var model = function() {
  return observeStates(randomValue(State), 0);
};
\end{lstlisting}
}

In this example, we use the unknown function \texttt{transFun} for state transitions and \texttt{obsFun} for observations.  This means that we will learn both the state transition matrix and the observation matrix.  Regret starts low and does not change over time, indicating that only one EM iteration was necessary to accurately estimate the parameters.

  \begin{figure}[h]
\begin{center}
  \includegraphics[scale=0.5]{../plots/accuracy_hmm.png}
\end{center}
\caption{Regret-per-sample quantiles for hidden Markov model}
\end{figure}

\FloatBarrier
\subsection{Latent Dirichlet allocation}
{\small
\begin{lstlisting}
var maxWordsPerDocument = 100;
var Topic = Categorical(3);
var Word = Categorical(10);
var topicToWord = randFunction(Topic, Word);

var model = function() {
  var whichClass = randomValue(Topic);
  var nWords = observe(randomIntegerERP, maxWordsPerDocument);
  repeat(nWords, function(wordIndex) {
    observe(classToWord, whichClass);
  });
};
\end{lstlisting}
}

For latent Dirichlet allocation, we use the unknown function \texttt{topicToWord} to map latent topics to word distributions.  We will learn a different categorical distribution for each topic.  Regret generally reduces over iterations.

  \begin{figure}[h]
\begin{center}
  \includegraphics[scale=0.5]{../plots/accuracy_lda.png}
\end{center}
\caption{Regret-per-sample quantiles for latent Dirichlet allocation}
\end{figure}

% \FloatBarrier
% \subsection{Neural network}
% 
% {\small
% \begin{lstlisting}
% var Input = Vector(30, Bool);
% var Hidden = Vector(10, Bool);
% var Output = Bool;
% 
% var inputToHidden = randFunction(Input, Hidden);
% var hiddenToOutput = randFunction(Hidden, Output);
% 
% var model = function() {
%   var inputLayer = observe(randomValue, Input);
%   var hiddenLayer = inputToHidden(inputs[sampIndex]);
%   observe(hiddenToOutput, hiddenLayer);
% };
% \end{lstlisting}
% }

  \section{Discussion}

  It is possible to write many useful machine learning models as Quipp programs and then use a generic Monte Carlo expectation maximization algorithm for inference.  This algorithm infers parameters reasonably well.  Non-experts will find it easier to write useful machine learning models in this language compared to other probabilistic programming lanvguages.
  
  In the future, it will be useful to support additional types.  For example, we could add types for standard exponential families such as Poisson and Gamma.  Supporting disjoint union types is also straightforward.  Also, it will be useful to create a more user-friendly interface for inferring parameters and performing additional data processing, such as inferring latent values given fixed parameters or visualizing the distributions implied by the parameters.

\bibliography{bibliography}{}
\bibliographystyle{plain}


\end{document}

